{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import cv2\n",
    "import torch\n",
    "import lpips\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import manual_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision.io import decode_image\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "CSV_FILE_PATH = './sardata_small.csv'\n",
    "IMAGE_DIR_SAR = './sardata_small/s1'\n",
    "IMAGE_DIR_COL = './sardata_small/s2'\n",
    "IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 1\n",
    "SEED = 42\n",
    "manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       s1_image  s1_image_path  s2_image  s2_image_path\n",
      "type                                                   \n",
      "agri        250            250       250            250\n",
      "urban       250            250       250            250\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "_, test_df = train_test_split(data_df, test_size=0.25, random_state=SEED, shuffle=True, stratify=data_df['type'])\n",
    "# print(train_df.groupby('type').count())\n",
    "print(test_df.groupby('type').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarColorDataset(Dataset):\n",
    "    def __init__(self, data_df, image_dir_sar, image_dir_col, transform_sar=None, transform_col=None):\n",
    "        self.data_df = data_df\n",
    "        self.image_dir_sar = image_dir_sar\n",
    "        self.image_dir_col = image_dir_col\n",
    "        self.transform_sar = transform_sar\n",
    "        self.transform_col = transform_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data_df.iloc[index]\n",
    "        label = row['type']\n",
    "\n",
    "        image_path_sar = os.path.join(self.image_dir_sar, row['s1_image'])\n",
    "        image_path_col = os.path.join(self.image_dir_col, row['s2_image'])\n",
    "\n",
    "        # image_sar = decode_image(image_path_sar, mode='GRAY')\n",
    "        # image_col = decode_image(image_path_col, mode='RGB')\n",
    "\n",
    "        # image_sar = cv2.imread(image_path_sar, cv2.IMREAD_GRAYSCALE)\n",
    "        # image_col = cv2.imread(image_path_col, cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "        image_sar = Image.open(image_path_sar).convert('L')\n",
    "        image_col = Image.open(image_path_col).convert('RGB')\n",
    "\n",
    "        if self.transform_sar:\n",
    "            image_sar = self.transform_sar(image_sar)\n",
    "\n",
    "        if self.transform_col:\n",
    "            image_col = self.transform_col(image_col)\n",
    "\n",
    "        return image_sar, image_col, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_sar = transforms.Compose([\n",
    "    transforms.Resize(size=IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,), inplace=False),\n",
    "])\n",
    "\n",
    "transform_col = transforms.Compose([\n",
    "    transforms.Resize(size=IMAGE_SIZE, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), inplace=False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SarColorDataset(\n",
    "    data_df=test_df,\n",
    "    image_dir_sar=IMAGE_DIR_SAR,\n",
    "    image_dir_col=IMAGE_DIR_COL,\n",
    "    transform_sar=transform_sar,\n",
    "    transform_col=transform_col,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, inp_c, out_c, kernel_size=4, stride=2, padding=1, use_bias=True, normalization='batch'):\n",
    "        super(DownSample, self).__init__()\n",
    "\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
    "        )\n",
    "\n",
    "        if (normalization == 'batch'):\n",
    "            self.down.append(nn.BatchNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        elif (normalization == 'instance'):\n",
    "            self.down.append(nn.InstanceNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False))\n",
    "\n",
    "        self.down.append(nn.LeakyReLU(negative_slope=0.2, inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    # def __init__(self, inp_c, out_c, kernel_size=4, stride=1, padding=0, use_bias = True, normalization='batch', apply_dropout=False, dropout_rate=0.5):\n",
    "    def __init__(self, inp_c, out_c, kernel_size=4, stride=2, padding=1, use_bias = True, normalization='batch', apply_dropout=False, dropout_rate=0.5):\n",
    "        super(UpSample, self).__init__()\n",
    "\n",
    "        # self.up = nn.Sequential(\n",
    "        #     nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "        #     nn.ZeroPad2d((2,1,2,1)),\n",
    "        #     nn.Conv2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
    "        # )\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=inp_c, out_channels=out_c, kernel_size=kernel_size, stride=stride, padding=padding, bias=(not normalization) and use_bias),\n",
    "        )\n",
    "\n",
    "        if (normalization == 'batch'):\n",
    "            self.up.append(nn.BatchNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        elif (normalization == 'isinstance'):\n",
    "            self.up.append(nn.InstanceNorm2d(num_features=out_c, eps=1e-5, momentum=0.1, affine=False, track_running_stats=False))\n",
    "\n",
    "        if apply_dropout:\n",
    "            self.up.append(nn.Dropout(p=dropout_rate, inplace=False))\n",
    "\n",
    "        self.up.append(nn.ReLU(inplace=False))\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.down_stack = nn.ModuleList([\n",
    "            DownSample(inp_c=in_channels, out_c=64, normalization=None),\n",
    "            DownSample(inp_c= 64, out_c=128),\n",
    "            DownSample(inp_c=128, out_c=256),\n",
    "            DownSample(inp_c=256, out_c=512),\n",
    "            DownSample(inp_c=512, out_c=512),\n",
    "            DownSample(inp_c=512, out_c=512),\n",
    "            DownSample(inp_c=512, out_c=512),\n",
    "            DownSample(inp_c=512, out_c=512, normalization=None),\n",
    "            ])\n",
    "\n",
    "        self.up_stack = nn.ModuleList([\n",
    "            UpSample(inp_c= 512, out_c=512), # removed dropout layers\n",
    "            UpSample(inp_c=1024, out_c=512),\n",
    "            UpSample(inp_c=1024, out_c=512),\n",
    "            UpSample(inp_c=1024, out_c=512),\n",
    "            UpSample(inp_c=1024, out_c=256),\n",
    "            UpSample(inp_c= 512, out_c=128),\n",
    "            UpSample(inp_c= 256, out_c= 64),\n",
    "            ])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.ZeroPad2d((2,1,2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=out_channels, kernel_size=4, stride=1, padding=0, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        skips = []\n",
    "        for layer in self.down_stack:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        skips.pop()\n",
    "        skips = skips[::-1]\n",
    "\n",
    "        for layer, skip in zip(self.up_stack, skips):\n",
    "            x = layer(x, skip)\n",
    "\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(in_channels=1, out_channels=3).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (down_stack): ModuleList(\n",
       "    (0): DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (1): DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (2): DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (3): DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (4-6): 3 x DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "    (7): DownSample(\n",
       "      (down): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LeakyReLU(negative_slope=0.2)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_stack): ModuleList(\n",
       "    (0): UpSample(\n",
       "      (up): Sequential(\n",
       "        (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1-3): 3 x UpSample(\n",
       "      (up): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): UpSample(\n",
       "      (up): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (5): UpSample(\n",
       "      (up): Sequential(\n",
       "        (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): UpSample(\n",
       "      (up): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (1): ZeroPad2d((2, 1, 2, 1))\n",
       "    (2): Conv2d(128, 3, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = './checkpoints/checkpoint_epoch_100.pth'\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nafis\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nafis\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\nafis\\AppData\\Roaming\\Python\\Python311\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_metric = lpips.LPIPS(net='alex').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./test_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "time_taken = 0.0\n",
    "\n",
    "for idx, (sar_image, col_image, _) in enumerate(test_dataloader):\n",
    "\n",
    "    sar_image = sar_image.to(DEVICE)\n",
    "    col_image = col_image.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        pred_image = generator(sar_image)\n",
    "        time_taken += time.time() - start\n",
    "    \n",
    "    def denormalize(tensor):\n",
    "        return (tensor * 0.5 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    for batch in range(BATCH_SIZE):\n",
    "\n",
    "        sar_image_np = denormalize(sar_image[batch]).permute(1, 2, 0).cpu().numpy().squeeze(-1)\n",
    "        pred_image_np = denormalize(pred_image[batch]).permute(1, 2, 0).cpu().numpy()\n",
    "        col_image_np = denormalize(col_image[batch]).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        psnr_value = psnr(col_image_np, pred_image_np, data_range=1.0)\n",
    "        ssim_value = ssim(col_image_np, pred_image_np, win_size=7, data_range=1.0, channel_axis=-1)\n",
    "        lpips_value = lpips_metric(pred_image[batch], col_image[batch]).item()\n",
    "\n",
    "        results.append([idx * BATCH_SIZE + batch, psnr_value, ssim_value, lpips_value])\n",
    "\n",
    "        # Save Images\n",
    "        sar_img = Image.fromarray((sar_image_np * 255).astype('uint8'))\n",
    "        pred_img = Image.fromarray((pred_image_np * 255).astype('uint8'))\n",
    "        col_img = Image.fromarray((col_image_np * 255).astype('uint8'))\n",
    "\n",
    "        sar_img.save(os.path.join(OUTPUT_DIR, f\"{idx * BATCH_SIZE + batch:03}_sar.png\"))\n",
    "        pred_img.save(os.path.join(OUTPUT_DIR, f\"{idx * BATCH_SIZE + batch:03}_pred.png\"))\n",
    "        col_img.save(os.path.join(OUTPUT_DIR, f\"{idx * BATCH_SIZE + batch:03}_color.png\"))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4.5))\n",
    "        fig.suptitle(f\"Image ID: {idx * BATCH_SIZE + batch:03} | PSNR: {psnr_value:.2f} | SSIM: {ssim_value:.2f} | LPIPS: {lpips_value:.4f}\")  # Adjust y to position title above\n",
    "\n",
    "        axes[0].imshow(sar_image_np, cmap=\"gray\")\n",
    "        axes[0].set_title(\"SAR Image\")\n",
    "        # axes[0].axis(\"off\")\n",
    "        \n",
    "        axes[1].imshow(pred_image_np)\n",
    "        axes[1].set_title(\"Predicted Image\")\n",
    "        # axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(col_image_np)\n",
    "        axes[2].set_title(\"Ground Truth\")\n",
    "        # axes[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f\"{idx * BATCH_SIZE + batch:03}_comparison.png\"))\n",
    "\n",
    "        if idx % 5 == 0:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "        results_df = pd.DataFrame(results, columns=[\"Index\", \"PSNR\", \"SSIM\", \"LPIPS\"])\n",
    "        results_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR: 21.99\n",
      "Average SSIM: 0.63\n",
      "Average LPIPS: 0.3059\n",
      "Average Prediction Time: 311.13 ms\n"
     ]
    }
   ],
   "source": [
    "avg_psnr = results_df[\"PSNR\"].mean()\n",
    "avg_ssim = results_df[\"SSIM\"].mean()\n",
    "avg_lpips = results_df[\"LPIPS\"].mean()\n",
    "avg_time = (time_taken * 1000) / (BATCH_SIZE * len(test_dataloader))\n",
    "\n",
    "print(f\"Average PSNR: {avg_psnr:.2f}\")\n",
    "print(f\"Average SSIM: {avg_ssim:.2f}\")\n",
    "print(f\"Average LPIPS: {avg_lpips:.4f}\")\n",
    "print(f\"Average Prediction Time: {avg_time:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
